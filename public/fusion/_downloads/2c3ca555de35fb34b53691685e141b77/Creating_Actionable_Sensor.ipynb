{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Creating an Actionable Sensor Example\nThis example demonstrates the process of creating an actionable sensor, i.e., a sensor that has an\nactionable property. This includes creation of :class:`~.Action` and :class:`~.ActionGenerator`\nthat will handle how this property can evolves over time, and how to interface with the sensor via\ngiven actions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Task\nWe will create a simple sensor with a field of view and infinite range. It lies on the 2D\nCartesian plane and can look in 1 of 4 directions: East, North, West, South.\nLet's call the attribute of the sensor that takes these values the `direction`, and model it\nsuch that it can instantaneously switch to a new value.\nWe'll define its field of view (FoV) as 90 degrees so that its observation of a particular\ndirection leads to it being completely blind of the areas observable in the other 3 directions.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Action\nThe logic of this `direction` switching will be handled by our custom :class:`~.Action`.\nThe class' `act` method contains the calculations needed to take in the property's current value\nand return its new value after a particular amount of time has elapsed.\n\nThis class inherits 3 important properties:\n* `generator` details which :class:`~.ActionGenerator` the action was created by. This will be\ndiscussed in detail later on.\n* `end_time` specifies when the action should be completed by. For example, we might want the\nsensor to switch from North to West in 5 seconds. So the end time would be 5 seconds from the\ncurrent time (the \"current time\" is a value stored by the sensor itself, and gets updated each\ntime it is called to action, which is a process discussed in detail later on). We will model the\naction behaviour so that the direction does not change value until the end time is reached. I.E\nwe only switch to West exactly when 5 seconds have elapsed, and not before.\n* `target_value` indicates what the new value of the property should be once the action is\ncomplete (end time is reached).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.sensor.action import Action\n\n\nclass ChangeDirectionAction(Action):\n    \"\"\"Simply changes the direction that the sensor is looking in when the action `end_time` is\n    reached.\"\"\"\n    def act(self, current_time, timestamp, init_value):\n        \"\"\"Only change to target direction once `end_time` is reached. Otherwise keep same value.\n\n        Parameters\n        ----------\n        current_time: datetime.timedelta\n            Current time (stored by sensor)\n        timestamp: datetime.timedelta\n            Modification of direction ends at this time stamp\n        init_value: Any\n            Current value of the direction\n\n        Returns\n        -------\n        Any\n            The new value of the direction\n        \"\"\"\n        if timestamp >= self.end_time:\n            return self.target_value  # target direction\n        else:\n            return init_value  # same direction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is within the :class:`~.Action` where you can detail more complicated modifications to the\nattribute. For example, the :class:`~.ChangeDwellAction` is an action for use with the\n`dwell_centre` property of the :class:`~.RadarRotatingBearingRange` sensor (or any other model\nwith similar dwell dynamics). It contains the logic of calculating the angle turned by the dwell\ncentre in the given time delta, given a constant rpm.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Action Generator\nNow that we have the logic of how the `direction` can change over time, we need to detail what\nthe potential changes can be for a given time frame. A :class:`~.ActionGenerator` type handles\nthe details of these potential property values.\nIn the more complicated dwell centre example above, this might be in determining what potential\nnew bearings are achievable in 5 seconds given a specific rpm.\nFor our `direction` example, we have a simpler task of deciding which directions are achievable\ngiven a particular time delta. As our \"switch\" is instantaneous, we clearly just yield an action\nfor every direction.\n\n5 important properties are inherited from this class:\n* `owner` specifies the sensor (or other :class:`Actionable`) that the corresponding property\nbelongs to. I.E. which sensor we will be querying.\n* `attribute` is the string-valued name of the property to be modified.\n* `start_time` is the time at which the sensor is queried and `end_time` the time at which it is\nqueried to. E.g. \"from now (9 o'clock), what can you do by 10 o'clock?\"\n* `current_value` details what the current value of the property is. In our case this would be\nwhat the current direction the sensor is looking in is.\n\nBy inheriting this class, we are required to define several things:\n* the `default_action` property determines what the behaviour of the property should be, given\nno actions have been passed to it (or that it has no actions to perform at the given time). For\nour `direction` example, we'll simply say that the direction won't change. So the default action\nshould be one of our `ChangeDirectionAction` types with a target value equal to the current value\nof the direction. For the dwell centre example discussed above, this might be reverting to a\ndefault, anti-clockwise rotation at the given rpm. The default action's end-time should last\nuntil the end of the query (i.e. until `end_time`).\n* the `__iter__` method defines how we calculate the potential actions for the sensor in the\ngiven time frame. We should be able to loop through this generator object and get out a\n`ChangeDirectionAction` for every potential new direction.\n* we should also define the `__contains__` method for this generator. This way, for a given\n`ChangeDirectionAction` or particular direction, we can say whether this is possible by simply\nasking \"is this IN my generator?\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.sensor.action import ActionGenerator\nfrom smartfusion.base import Property\nfrom smartfusion.sensor.sensor import Sensor\n\n\nclass DirectionActionsGenerator(ActionGenerator):\n    \"\"\"Return an action for each possible direction that the sensor can look at by `end_time`.\"\"\"\n    owner: Sensor = Property(\n        doc=\"Sensor with `timestamp`, `direction` and `potential_directions` attributes\")\n\n    @property\n    def default_action(self):\n        \"\"\"The default action is to remain \"looking\" in the same direction, so the `target_value`\n        is the same as the `current_value`.\"\"\"\n        return ChangeDirectionAction(generator=self,\n                                     end_time=self.end_time,\n                                     target_value=self.current_value)\n\n    def __contains__(self, item):\n        \"\"\"Can switch to any direction in any time frame (as long as it is sensible. eg. we\n        shouldn't expect to be able to look in the direction \"up\" or \"weast\").\"\"\"\n        if isinstance(item, ChangeDirectionAction):\n            item = item.target_value  # grab the target value of the action to check against\n\n        potential_directions = self.owner.potential_directions\n\n        return item in potential_directions  # if its a potential direction, then it is possible\n\n    def __iter__(self):\n        \"\"\"\n        yield an action for every potential direction that is possible to look at in time frame.\n        \"\"\"\n        for direction in self.owner.potential_directions:\n            yield ChangeDirectionAction(generator=self,\n                                        end_time=self.end_time,\n                                        target_value=direction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Actionable Sensor\nAll sensor models in SMART FUSION inherit from the :class:`~.Actionable` type. This class handles\nthe logic needed for dealing with being actioned (i.e. receiving :class:`~.Action` sets in some\nmanner and applying them to their corresponding properties).\nTo flag a particular property as something which should be actioned, simply define it as an\n:class`~.ActionableProperty` and provide a :class:`ActionGenerator` so that it is clear how it\nshould be modified over time.\n\nAn :class:`~.Actionable` keeps track of its \"schedule\" via a dictionary, keyed by the actionable\nproperties' names, where values are the most recent actions attributed to those properties. In\nthe instance where a key has corresponding value/action that has been completed, the `act` method\nwill handle the removal of this action from the schedule dictionary. An important point to take\nhere is that only a single action can be scheduled per property.\n\nThere are 3 important methods inherited by :class:`~.Actionable`:\n* `actions` will return a set of action generators (one for each actionable property of the\nsensor). The method requires a timestamp to be passed in such that the generators have an end\ntime to calculate their possibilities for.\n* `add_actions` takes a sequence of provided actions and adds them as values to their\ncorresponding actionable properties in the sensor's schedule dictionary. This will overwrite any\npre-existing actions for those properties.\n* `act` handles the actual calling to each of the scheduled actions for every actionable\nproperty. A timestamp is required so that the sensor knows what time to act until. For every\nactionable property the sensor has, this method calls the property's corresponding scheduled\naction, if it has one, up until the timestamp has been reached, and replaces the property's value\nwith the new calculated one. The method also handles situations where the given timestamp\noverruns the end time of scheduled actions, whereby it will revert to calling the property's\ncorresponding generator's default action for the rest of the time needed to reach the timestamp.\n\nAn :class:`~.Actionable` type requires a method to `validate_timestamp` in order to keep track\nof what it should consider as \"now\". In the case of :class:`~.sensor` types, this is done by\ntaking the sensor's corresponding movement controller's timestamp.\n\nThere will be some logic in calculating whether a target falls within the sensor's FoV at a given\ntime, but the important point to take from this model is the creation of\n:class:`~.ActionableProperty` types for properties that you would like to be actionable.\nIn the instance where a sensor has no actionable properties, it simply is not affected by being\ngiven a set of actions, nor will yield any action generators when queried, and won't do anything\nwhen called to `act`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from numpy import pi\n\nfrom smartfusion.sensor.sensor import Sensor\nfrom smartfusion.sensor.actionable import ActionableProperty\nfrom smartfusion.models.measurement.linear import LinearGaussian\nfrom smartfusion.models.measurement.nonlinear import Cartesian2DToBearing\nfrom smartfusion.types.detection import TrueDetection\nfrom smartfusion.types.array import StateVector\n\n\nclass DirectedSensor(Sensor):\n    direction: str = ActionableProperty(doc=\"Direction that sensor is looking in\",\n                                        generator_cls=DirectionActionsGenerator)\n\n    @property\n    def potential_directions(self):\n        return [\"East\", \"North\", \"West\", \"South\"]\n\n    @property\n    def potential_directions_angles(self):\n        \"\"\"Need a slightly more meaningful definition of direction when calculating what falls in\n        to the sensor's FoV.\"\"\"\n        return {string: angle for string, angle\n                in zip(self.potential_directions, [.0, pi/2, pi, 3*pi/2])}\n\n    @property\n    def direction_angle(self):\n        return StateVector([0, 0, self.potential_directions_angles[self.direction]])\n\n    @property\n    def measurement_model(self):\n        return LinearGaussian(ndim_state=4,\n                              mapping=(0, 2),\n                              noise_covar=np.eye(2))\n\n    def measure(self, ground_truths, noise=None, **kwargs):\n\n        detections = set()\n        for truth in ground_truths:\n\n            bearing_calculator = Cartesian2DToBearing(ndim_state=4,\n                                                      mapping=(0, 2),\n                                                      noise_covar=None,\n                                                      translation_offset=self.position,\n                                                      rotation_offset=self.direction_angle)\n            bearing = bearing_calculator.function(truth, noise=False)\n\n            # Do not measure if state not in FoV (90 degrees)\n            if bearing > pi/4 or bearing < -pi/4:\n                continue\n\n            measurement_vector = self.measurement_model.function(truth, noise=False, **kwargs)\n            detection = TrueDetection(measurement_vector,\n                                      measurement_model=self.measurement_model,\n                                      timestamp=truth.timestamp,\n                                      groundtruth_path=truth)\n            detections.add(detection)\n\n        return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Flow\n\n<img src=\"https://smartfusion.rtfd.io/en/latest/_static/actionable_sensor_flow_diagram.png\" width=\"500\" alt=\"Actionable sensor flow diagram\">\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Platform\nTo demonstrate the sensor in action, we will use a :class:`~.PlatformDetectionSimulator` (the\nsensor's timestamp will also be attained from the platform's movement controller).\nWe'll start the sensor off by looking \"North\", centred at the origin.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import datetime\nfrom smartfusion.platform.base import FixedPlatform\nfrom smartfusion.types.state import State\n\nnow = datetime.datetime.now()\n\nsensor = DirectedSensor(direction=\"North\")  # sensor starts by looking \"North\"\n\n# Need to fix sensor to platform for use in detection simulator\nplatform = FixedPlatform(position_mapping=(0, 2),\n                         states=[State([0, 0, 0, 0], timestamp=now)],\n                         sensors=[sensor])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Methods\nWe can query what the sensor is capable of in the next 5 seconds with the following method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "five_sec_actions = DirectedSensor(direction=\"North\").actions(now + datetime.timedelta(seconds=5))\nfive_sec_actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This method returns a set of :class:`~.ActionGenerator`. As the sensor has one actionable\nproperty (`direction`), a single :class:`DirectionActionsGenerator` is returned.\nThe generator's default action is to remain looking in the same direction (North):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "generator = five_sec_actions.pop()\ngenerator.default_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Iterating through the generator yields all potential actions the sensor can take in 5 seconds:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for action in generator:\n    print(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can schedule a particular action for the `direction` property simply by passing one of these\nactions to the `add_actions` method (returning `True` if the action is successfully scheduled):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sensor.add_actions({action})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sensor.scheduled_actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we now action the sensor for 2 seconds, the action we have scheduled for `direction` will be\ncalled to `act` for 2 seconds and replace the `direction` with a new value. Since we programmed\nthe :class:`ChangeDirectionAction` to not change anything until the end-time (5 seconds later)\nis reached, we should not see any change in `direction`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Timestamp at 0s: \", sensor.timestamp)\nprint(\"Direction at 0s: \", sensor.direction)\n\nsensor.act(now + datetime.timedelta(seconds=2))\n\nprint(\"Timestamp at 2s: \", sensor.timestamp)\nprint(\"Direction at 2s: \", sensor.direction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we act until all 5 seconds have elapsed, we should see the sensor change to a new\ndirection:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sensor.act(now + datetime.timedelta(seconds=5))\nprint(\"Timestamp at 5s: \", sensor.timestamp)\nprint(\"Direction at 5s: \", sensor.direction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating simulator\nWe'll create a simple ground truth simulation on the 2D Cartesian plane, with a state space\ncontaining velocities. I.E. a state vector will be of the form $(x, \\dot{x}, y, \\dot{y})$.\n\nThe sensor starts the simulation by looking North. After 20 iterations of our simulation, we\nwill query what is possible in 5 seconds. From the resultant generator, we will take the last\npotential action (which should be to look South), and schedule this. Then, after another 20\niterations, query what is possible in another 5 seconds, and take the action directing the\nsensor East (the first yielded action from the generator).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.models.transition.linear import ConstantVelocity, \\\n    CombinedLinearGaussianTransitionModel\nfrom smartfusion.types.state import GaussianState\nimport datetime\nimport numpy as np\nfrom smartfusion.simulator.simple import MultiTargetGroundTruthSimulator\nfrom smartfusion.simulator.platform import PlatformDetectionSimulator\n\n\ntransition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(0.1),\n                                                          ConstantVelocity(0.1)])\nnow = datetime.datetime.now()\ninitial_state = GaussianState([0, 0, 0, 0], covar=np.diag([25, 1, 25, 1]), timestamp=now)\ngroundtruth_sim = MultiTargetGroundTruthSimulator(transition_model,\n                                                  initial_state,\n                                                  number_steps=100)\n\nsensor = DirectedSensor(direction=\"North\")  # sensor starts by looking \"North\"\n\n# Need to fix sensor to platform for use in detection simulator\nplatform = FixedPlatform(position_mapping=(0, 2),\n                         states=[State([0, 0, 0, 0], timestamp=now)],\n                         sensors=[sensor])\n\ndetector = PlatformDetectionSimulator(groundtruth_sim, {platform})\n\ngroundtruths = set()\nall_detections = set()\n\nfor time_index, (time, detections) in enumerate(detector, 1):\n\n    if time_index == 20:\n        generator = sensor.actions(time + datetime.timedelta(seconds=5)).pop()\n        for action in generator:  # loop through generator, get last action (South)\n            pass\n        sensor.add_actions({action})\n    elif time_index == 40:\n        generator = sensor.actions(time + datetime.timedelta(seconds=5)).pop()\n        action = next(iter(generator))  # get first action (East)\n        sensor.add_actions({action})\n    groundtruths.update(groundtruth_sim.current[1])\n    all_detections.update(detections)\n\nprint(f\"num ground truths = {len(groundtruths)}, num detections = {len(all_detections)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nfig.suptitle(\"Detections of Directed Sensor\", fontsize=16)\nax.figure.set_size_inches(20, 20)\n\n# Plot sensor possible FoVs\nfor bearing in (pi/4, 3*pi/4, 5*pi/4, 7*pi/4):\n    ax.plot((0, 100*np.cos(bearing)), (0, 100*np.sin(bearing)), c=\"green\")\n\n# Plot sensor location\nax.scatter(0, 0, s=100, c=\"black\")\n\n# Plot ground truths\nfor truth in groundtruths:\n    ax.plot(*np.array([state.state_vector[(0, 2), :].flatten() for state in truth]).T, c=\"blue\")\n    ax.figure.set_size_inches(20, 20)\n\n# Plot detections\nfor detection in all_detections:\n    ax.scatter(*detection.state_vector, c=\"red\", marker=\"*\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}