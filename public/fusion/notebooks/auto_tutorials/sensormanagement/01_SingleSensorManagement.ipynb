{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 1 - Single Sensor Management\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial introduces the sensor manager classes in SMART FUSION which can be used to build simple sensor management\nalgorithms for tracking and state estimation. The intention is to further build on these base classes to develop more\ncomplex sensor management algorithms.\n\n## Background\n\nSensor management is the process of deciding and executing the actions that a sensor, or group of sensors\nwill take in a specific scenario and with a particular objective, or objectives in mind. The process\ninvolves using information about the scenario to determine an appropriate action for the sensing system\nto take. An observation of the state of the system is then made using the sensing configuration decided\nby the sensor manager. The observations are used to update the estimate of the collective states and this\nupdate is used (if necessary) to determine the next action for the sensing system to take.\n\nA simple example can be imagined using a sensor with a limited field of view which must decide which direction\nit should point in at each time step. Alternatively, we might construct an objective based example by imagining\nthat the desired target is fast moving and the sensor can only observe one target at a time. If there are\nmultiple targets which could be observed the sensor manager could choose to observe the target that had the\ngreatest estimated velocity at the current time.\n\nThe example in this notebook considers two simple sensor management methods and applies them to the same\nground truths in order to quantify the difference in behaviour. The scenario simulates 3 targets moving on\nnearly constant velocity trajectories and a radar with a specified field of view, which can be pointed in a\nparticular direction.\n\nThe first method, using the class :class:`~.RandomSensorManager` chooses a direction to point in randomly\nwith equal probability. The\nsecond method, using the class :class:`~.BruteForceSensorManager` considers every possible direction the\nsensor could point in and uses a\nreward function to determine the best choice of action.\nIn this example the reward function aims to reduce the total uncertainty of the track estimates at each time step.\nTo achieve this the sensor manager chooses to look in the direction which results in the greatest reduction in\nuncertainty - as represented by\nthe Frobenius norm of the covariance matrix.\n\n### Sensor management as a POMDP\n\nSensor management problems can be considered as Partially Observable Markov Decision Processes (POMDPs) where\nobservations provide information about the current state of the system but there is uncertainty in the estimate\nof the underlying state due to noisy sensors and imprecise models of target evaluation.\n\nPOMDPs consist of:\n * $X_k$, the finite set of possible states for each stage index $k$.\n * $A_k$, the finite set of possible actions for each stage index $k$.\n * $R_k(x, a)$, the reward function.\n * $Z_k$, the finite set of possible observations for each stage index $k$.\n * $f_k(x_{k}|x_{k-1})$, a (set of) state transition function(s). (Note that actions are excluded from\n   the function at the moment. It may be necessary to include them if prior sensor actions cause the targets to\n   modify their behaviour.)\n * $h_k(z_k | x_k, a_k)$, a (set of) observation function(s).\n * $\\{x\\}_k$, the set of states at $k$ to be estimated.\n * $\\{a\\}_k$, a set of actions at $k$ to be chosen.\n * $\\{z\\}_k$, the observations at $k$ returned by the sensor.\n * $\\Psi_{k-1}$, denotes the complete set of 'intelligence' available to the sensor manager before deciding\n   on an action at $k$. This includes the prior set of state estimates $\\{x\\}_{k-1}$, but may also\n   encompass contextual information, sensor constraints or mission parameters.\n\nFigure 1: Illustration of sequential actions and measurements. [#]_\n\n<img src=\"https://smartfusion.rtfd.io/en/latest/_static/SM_flow_diagram.png\" width=\"800\" alt=\"Illustration of sequential actions and measurements\">\n\n$\\Psi_k$ is the intelligence available to the sensor manager at stage index $k$, to help\nselect the action $a_k$ for the system to take. An observation $z_k$ is made by the sensing system,\ngiving information on the state $x_k$. The action $a_k$ and observation $z_k$ are added to the\nintelligence set to generate $\\Psi_{k+1}$, the intelligence available at stage index $k+1$.\n\n### Comparing sensor management methods using metrics\n\nThe performance of the two sensor management methods explored in this tutorial can be assessed using metrics\navailable from the SMART FUSION framework. The metrics used to assess the performance of the different methods\nare the OPSA metric [#]_, SIAP metrics [#]_ and an uncertainty metric. Demonstration of the OSPA and SIAP metrics\ncan be found in the Metrics Example.\n\nThe uncertainty metric computes the covariance matrices of all target states at each time step and calculates the\nsum of their norms. This gives a measure of the total uncertainty across all tracks at each time step.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sensor Management example\n\n### Setup\n\nFirst a simulation must be set up using components from SMART FUSION. For this the following imports are required.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\n\nstart_time = datetime.now()\n\nfrom smartfusion.models.transition.linear import CombinedLinearGaussianTransitionModel, ConstantVelocity\nfrom smartfusion.types.groundtruth import GroundTruthPath, GroundTruthState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate ground truths\n\nFollowing the methods from previous SMART FUSION tutorials we generate a series of combined linear Gaussian transition\nmodels and generate ground truths. Each ground truth is offset in the y-direction by 10.\n\nThe number of targets in this simulation is defined by `ntruths` - here there are 3 targets travelling in different\ndirections. The time the simulation is observed for is defined by `time_max`.\n\nWe can fix our random number generator in order to probe a particular example repeatedly. This can be undone by\ncommenting out the first two lines in the next cell.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1990)\nrandom.seed(1990)\n\n# Generate transition model\n# i.e. fk(xk|xk-1)\ntransition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(0.005),\n                                                          ConstantVelocity(0.005)])\n\nyps = range(0, 100, 10)  # y value for prior state\ntruths = []\nntruths = 3  # number of ground truths in simulation\ntime_max = 50  # timestamps the simulation is observed over\n\nxdirection = 1\nydirection = 1\n\n# Generate ground truths\nfor j in range(0, ntruths):\n    truth = GroundTruthPath([GroundTruthState([0, xdirection, yps[j], ydirection], timestamp=start_time)],\n                            id=f\"id{j}\")\n\n    for k in range(1, time_max):\n        truth.append(\n            GroundTruthState(transition_model.function(truth[k - 1], noise=True, time_interval=timedelta(seconds=1)),\n                             timestamp=start_time + timedelta(seconds=k)))\n    truths.append(truth)\n\n    # alternate directions when initiating tracks\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the ground truths. This is done using the :class:`~.Plotterly` class from SMART FUSION.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.plotter import Plotterly\n\n# Smartfusion plotter requires sets not lists\ntruths_set = set(truths)\n\nplotter = Plotterly()\nplotter.plot_ground_truths(truths_set, [0, 2])\nplotter.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create sensors\n\nCreate a sensor for each sensor management algorithm. This tutorial uses the\n:class:`~.RadarRotatingBearingRange` sensor. This sensor is an :class:`~.Actionable` so\nis capable of returning the actions it can possibly\ntake at a given time step and can also be given an action to take before taking\nmeasurements.\nSee the Creating an Actionable Sensor Example for a more detailed explanation of actionable sensors.\n\nThe :class:`~.RadarRotatingBearingRange` has a dwell centre which is an :class:`~.ActionableProperty`\nso in this case the action is changing the dwell centre to point in a specific direction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.types.state import StateVector\nfrom smartfusion.sensor.radar.radar import RadarRotatingBearingRange\n\nsensorA = RadarRotatingBearingRange(\n    position_mapping=(0, 2),\n    noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                          [0, 1 ** 2]]),\n    ndim_state=4,\n    position=np.array([[10], [0]]),\n    rpm=60,\n    fov_angle=np.radians(30),\n    dwell_centre=StateVector([0.0]),\n    max_range=np.inf\n)\nsensorA.timestamp = start_time\n\nsensorB = RadarRotatingBearingRange(\n    position_mapping=(0, 2),\n    noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                          [0, 1 ** 2]]),\n    ndim_state=4,\n    position=np.array([[10], [0]]),\n    rpm=60,\n    fov_angle=np.radians(30),\n    dwell_centre=StateVector([0.0]),\n    max_range=np.inf\n)\nsensorB.timestamp = start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Kalman predictor and updater\n\nConstruct a predictor and updater using the :class:`~.KalmanPredictor` and :class:`~.ExtendedKalmanUpdater`\ncomponents from SMART FUSION. The :class:`~.ExtendedKalmanUpdater` is used because it can be used for both linear\nand nonlinear measurement models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.predictor.kalman import KalmanPredictor\npredictor = KalmanPredictor(transition_model)\n\nfrom smartfusion.updater.kalman import ExtendedKalmanUpdater\nupdater = ExtendedKalmanUpdater(measurement_model=None)\n# measurement model is added to detections by the sensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Kalman filters\n\nFirst create `ntruths` priors which estimate the targets\u2019 initial states, one for each target. In this example\neach prior is offset by 0.5 in the y direction meaning the position of the track is initially not very accurate. The\nvelocity is also systematically offset by +0.5 in both the x and y directions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.types.state import GaussianState\n\npriors = []\nxdirection = 1.2\nydirection = 1.2\nfor j in range(0, ntruths):\n    priors.append(GaussianState([[0], [xdirection], [yps[j]+0.1], [ydirection]],\n                                np.diag([0.5, 0.5, 0.5, 0.5]+np.random.normal(0,5e-4,4)),\n                                timestamp=start_time))\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise the tracks by creating an empty list and appending the priors generated. This needs to be done separately\nfor both sensor manager methods as they will generate different sets of tracks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.types.track import Track\n\n# Initialise tracks from the RandomSensorManager\ntracksA = []\nfor j, prior in enumerate(priors):\n    tracksA.append(Track([prior]))\n\n# Initialise tracks from the BruteForceSensorManager\ntracksB = []\nfor j, prior in enumerate(priors):\n    tracksB.append(Track([prior]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create sensor managers\n\nNext we create our sensor manager classes. Two sensor manager classes are used in this tutorial\n- :class:`~.RandomSensorManager` and :class:`~.BruteForceSensorManager`.\n\n#### Random sensor manager\n\nThe first method :class:`~.RandomSensorManager`, randomly chooses the action(s) for the sensor to take\nto make an observation. To do this the :meth:`choose_actions`\nfunction uses :meth:`random.sample()` to draw a random sample from all possible directions the sensor could point in\nat each time step.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.sensormanager import RandomSensorManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Brute force sensor manager\n\nThe second method :class:`~.BruteForceSensorManager` iterates through every possible action a sensor can take at a\ngiven time step and selects the action(s) which give the maximum reward as calculated by the reward function.\nIn this example the reward function is used to select a direction for the sensor to point in\nsuch that the total uncertainty of the tracks will be\nreduced the most by making an observation in that direction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.sensormanager import BruteForceSensorManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reward function\nA reward function is used to quantify the benefit of sensors taking a particular action or set of actions.\nThis can be crafted specifically for an example in order to achieve a particular objective. The function used in\nthis example is quite generic but could be substituted for any callable function which returns a numeric\nvalue that the sensor manager can maximise.\n\nThe :class:`~.UncertaintyRewardFunction` calculates the uncertainty reduction by computing the difference between the\ncovariance matrix norms of the\nprediction, and the posterior assuming a predicted measurement corresponding to that prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.sensormanager.reward import UncertaintyRewardFunction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initiate the sensor managers\n\nCreate an instance of each sensor manager class. Each class takes in a `sensor_set`, for this example\nit is a set of one sensor.\nThe :class:`~.BruteForceSensorManager` also requires a callable reward function which here is the\n:class:`UncertaintyRewardFunction`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "randomsensormanager = RandomSensorManager({sensorA})\n\n# initiate reward function\nreward_function = UncertaintyRewardFunction(predictor, updater)\n\nbruteforcesensormanager = BruteForceSensorManager({sensorB},\n                                                  reward_function=reward_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the sensor managers\n\nFor both methods the :meth:`choose_actions`\nfunction requires a time step and a tracks list as inputs.\n\nFor both sensor management methods, the chosen actions are added to the sensor and measurements made.\nTracks which have been observed by the sensor are updated and those that haven't are predicted forward.\nThese states are appended to the tracks list.\n\nFirst a hypothesiser and data associator are required for use in both trackers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.hypothesiser.distance import DistanceHypothesiser\nfrom smartfusion.measures import Mahalanobis\nhypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=5)\n\nfrom smartfusion.dataassociator.neighbour import GNNWith2DAssignment\ndata_associator = GNNWith2DAssignment(hypothesiser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run random sensor manager\n\nHere the chosen target for observation is selected randomly using the method :meth:`choose_actions()` from the class\n:class:`~.RandomSensorManager`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ordered_set import OrderedSet\n\n# Generate list of timesteps from ground truth timestamps\ntimesteps = []\nfor state in truths[0]:\n    timesteps.append(state.timestamp)\n\nfor timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    # i.e. {a}k\n    chosen_actions = randomsensormanager.choose_actions(tracksA, timestep)\n\n    # Create empty dictionary for measurements\n    measurementsA = []\n\n    for chosen_action in chosen_actions:\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(actions)\n\n    sensorA.act(timestep)\n\n    # Observe this ground truth\n    # i.e. {z}k\n    measurements = sensorA.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n    measurementsA.extend(measurements)\n\n    hypotheses = data_associator.associate(tracksA,\n                                           measurementsA,\n                                           timestep)\n    for track in tracksA:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotterA = Plotterly()\nplotterA.plot_sensors(sensorA)\nplotterA.plot_ground_truths(truths_set, [0, 2])\nplotterA.plot_tracks(set(tracksA), [0, 2], uncertainty=True)\nplotterA.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run brute force sensor manager\n\nHere the chosen action is selected based on the difference between the\ncovariance matrices of the\nprediction and posterior, for targets which could be observed by the sensor taking that action\n- i.e. pointing it's dwell centre in that given direction.\n\nThe :meth:`choose_actions` function from the :class:`~.BruteForceSensorManager` is called at each time step.\nThis means that at each time step, for each track:\n\n * A prediction is made for each track and the covariance matrix norms stored\n * For each possible action a sensor could take, a synthetic detection is made using this sensor configuration\n * A hypothesis is generated based on the stored prediction and synthetic detection\n * This hypothesis is used to do an update and the covariance matrix norms of the update are stored\n * The difference between the covariance matrix norms of the update and the prediction is calculated\n\nThe overall reward is calculated as the sum of the differences between these covariance matrix norms\nfor all the tracks observed by the possible action. The sensor manager identifies the action which results\nin the largest value of this reward and therefore largest reduction in uncertainty and returns the optimal\nsensor/action mapping.\n\nThe chosen action is given to the sensor, measurements are made and the tracks updated based on these measurements.\nPredictions are made for tracks which have not been observed by the sensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    # i.e. {a}k\n    chosen_actions = bruteforcesensormanager.choose_actions(tracksB, timestep)\n\n    # Create empty dictionary for measurements\n    measurementsB = []\n\n    for chosen_action in chosen_actions:\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(actions)\n\n    sensorB.act(timestep)\n\n    # Observe this ground truth\n    # i.e. {z}k\n    measurements = sensorB.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n    measurementsB.extend(measurements)\n\n    hypotheses = data_associator.associate(tracksB,\n                                           measurementsB,\n                                           timestep)\n    for track in tracksB:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotterB = Plotterly()\nplotterB.plot_sensors(sensorB)\nplotterB.plot_ground_truths(truths_set, [0, 2])\nplotterB.plot_tracks(set(tracksB), [0, 2], uncertainty=True)\nplotterB.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The smaller uncertainty ellipses in this plot suggest that the :class:`~.BruteForceSensorManager` provides a much\nbetter track than the :class:`~.RandomSensorManager`.\n\n## Metrics\n\nMetrics can be used to compare how well different sensor management techniques are working.\nFull explanations of the OSPA\nand SIAP metrics can be found in the Metrics Example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.metricgenerator.ospametric import OSPAMetric\nospa_generator = OSPAMetric(c=40, p=1)\n\nfrom smartfusion.metricgenerator.tracktotruthmetrics import SIAPMetrics\nfrom smartfusion.measures import Euclidean\nsiap_generator = SIAPMetrics(position_measure=Euclidean((0, 2)),\n                             velocity_measure=Euclidean((1, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SIAP metrics require an associator to associate tracks to ground truths. This is done using the\n:class:`~.TrackToTruth` associator with an association threshold of 30.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.dataassociator.tracktotrack import TrackToTruth\nassociator = TrackToTruth(association_threshold=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The OSPA and SIAP metrics don't take the uncertainty of the track into account. The initial plots of the\ntracks and ground truths show by plotting the uncertainty ellipses that there is generally less uncertainty\nin the tracks generated by the :class:`~.BruteForceSensorManager`.\n\nTo capture this we can use an uncertainty metric to look at the sum of covariance matrix norms at\neach time step. This gives a representation of the overall uncertainty of the tracking over time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.metricgenerator.uncertaintymetric import SumofCovarianceNormsMetric\nuncertainty_generator = SumofCovarianceNormsMetric()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A metric manager is used for the generation of metrics on multiple :class:`~.GroundTruthPath` and\n:class:`~.Track` objects. This takes in the metric generators, as well as the associator required for the\nSIAP metrics.\n\nWe must use a different metric manager for each sensor management method. This is because each sensor manager\ngenerates different track data which is then used in the metric manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.metricgenerator.manager import SimpleManager\n\nmetric_managerA = SimpleManager([ospa_generator, siap_generator, uncertainty_generator],\n                                associator=associator)\n\nmetric_managerB = SimpleManager([ospa_generator, siap_generator, uncertainty_generator],\n                                associator=associator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each time step, data is added to the metric manager on truths and tracks. The metrics themselves can then be\ngenerated from the metric manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metric_managerA.add_data(truths, tracksA)\nmetric_managerB.add_data(truths, tracksB)\n\nmetricsA = metric_managerA.generate_metrics()\nmetricsB = metric_managerB.generate_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OSPA metric\n\nFirst we look at the OSPA metric. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nospa_metricA = metricsA['OSPA distances']\nospa_metricB = metricsB['OSPA distances']\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.plot([i.timestamp for i in ospa_metricA.value],\n        [i.value for i in ospa_metricA.value],\n        label='RandomSensorManager')\nax.plot([i.timestamp for i in ospa_metricB.value],\n        [i.value for i in ospa_metricB.value],\n        label='BruteForceSensorManager')\nax.set_ylabel(\"OSPA distance\")\nax.set_xlabel(\"Time\")\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~.BruteForceSensorManager` generally results in a smaller OSPA distance\nthan the random observations of the :class:`~.RandomSensorManager`, reflecting the better tracking performance\nseen in the tracking plots.\n\n### SIAP metrics\n\nNext we look at SIAP metrics. This can be done by generating a table which displays all the SIAP metrics computed,\nas seen in the Metrics Example.\nHowever, completeness, ambiguity and spuriousness are not relevant for this example because we are not initiating and\ndeleting tracks and we have one track corresponding to each ground truth. Here we only plot positional accuracy and\nvelocity accuracy over time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2)\n\ntimes = metric_managerA.list_timestamps()\n\npa_metricA = metricsA['SIAP Position Accuracy at times']\nva_metricA = metricsA['SIAP Velocity Accuracy at times']\n\npa_metricB = metricsB['SIAP Position Accuracy at times']\nva_metricB = metricsB['SIAP Velocity Accuracy at times']\n\naxes[0].set(title='Positional Accuracy', xlabel='Time', ylabel='PA')\naxes[0].plot(times, [metric.value for metric in pa_metricA.value],\n             label='RandomSensorManager')\naxes[0].plot(times, [metric.value for metric in pa_metricB.value],\n             label='BruteForceSensorManager')\naxes[0].legend()\n\naxes[1].set(title='Velocity Accuracy', xlabel='Time', ylabel='VA')\naxes[1].plot(times, [metric.value for metric in va_metricA.value],\n             label='RandomSensorManager')\naxes[1].plot(times, [metric.value for metric in va_metricB.value],\n             label='BruteForceSensorManager')\naxes[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the OSPA distances the :class:`~.BruteForceSensorManager`\ngenerally results in both a better positional accuracy and velocity accuracy than the random observations\nof the :class:`~.RandomSensorManager`.\n\n### Uncertainty metric\n\nFinally we look at the uncertainty metric which computes the sum of covariance matrix norms of each state at each\ntime step. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "uncertainty_metricA = metricsA['Sum of Covariance Norms Metric']\nuncertainty_metricB = metricsB['Sum of Covariance Norms Metric']\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.plot([i.timestamp for i in uncertainty_metricA.value],\n        [i.value for i in uncertainty_metricA.value],\n        label='RandomSensorManager')\nax.plot([i.timestamp for i in uncertainty_metricB.value],\n        [i.value for i in uncertainty_metricB.value],\n        label='BruteForceSensorManager')\nax.set_ylabel(\"Sum of covariance matrix norms\")\nax.set_xlabel(\"Time\")\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This metric shows that the uncertainty in the tracks generated by the :class:`~.RandomSensorManager` is much greater\nthan for those generated by the :class:`~.BruteForceSensorManager`. This is also reflected by the uncertainty ellipses\nin the initial plots of tracks and truths.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [#] *Hero, A.O., Castanon, D., Cochran, D. and Kastella, K.*, **Foundations and Applications of Sensor\n   Management**. New York: Springer, 2008.\n.. [#] *D. Schuhmacher, B. Vo and B. Vo*, **A Consistent Metric for Performance Evaluation of\n   Multi-Object Filters**, IEEE Trans. Signal Processing 2008\n.. [#] *Votruba, Paul & Nisley, Rich & Rothrock, Ron and Zombro, Brett.*, **Single Integrated Air\n   Picture (SIAP) Metrics Implementation**, 2001\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}