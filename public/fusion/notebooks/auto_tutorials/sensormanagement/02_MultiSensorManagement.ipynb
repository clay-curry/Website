{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 2 - Multiple Sensor Management\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial follows on from the Single Sensor Management tutorial and further explores how existing\nSMART FUSION features can be used to build simple sensor management algorithms for tracking and\nstate estimation. This second tutorial demonstrates the limitations of the brute force optimisation\nmethod introduced in the previous tutorial by increasing the number of sensors used in the scenario.\n\n## Introducing multiple sensors\nThe example in this tutorial considers the same sensor management methods as in Tutorial 1 and applies them to the\nsame set of ground truths in order to observe the difference in tracks. The scenario simulates 3\ntargets moving on nearly constant velocity trajectories and in this case an adjustable number of sensors.\nThe sensors are\nsimple radar with a defined field of view which can be pointed in a particular direction in order\nto make an observation.\n\nThe first method, using the class :class:`~.RandomSensorManager` chooses a target for each sensor to\nobserve randomly with equal probability.\n\nThe second method, uses the class :class:`~.BruteForceSensorManager` and aims to reduce the total\nuncertainty of the track estimates at each\ntime step. To achieve this the sensor manager considers all possible configurations of directions for the sensors\nto point in. The sensor manager chooses the configuration for which the sum of estimated\nuncertainties (as represented by the Frobenius norm of the covariance matrix) can be reduced the most by observing\nthe targets within the chosen sensing configuration.\n\nThe introduction of multiple sensors means an increase in the possible combinations of action choices\nthat the brute force sensor manager must consider. This brute force optimisation method of looking at every possible\ncombination of actions becomes very slow as more sensors are introduced, demonstrating the\nlimitations of using this method in more complex scenarios.\n\nAs in the first tutorial the OSPA [#]_, SIAP [#]_ and uncertainty metrics are used to assess the performance of the\nsensor managers.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sensor Management example\n\n### Setup\n\nFirst a simulation must be set up using components from SMART FUSION. For this the following imports are required.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport random\nfrom datetime import datetime, timedelta\n\nstart_time = datetime.now()\n\nfrom smartfusion.models.transition.linear import CombinedLinearGaussianTransitionModel, ConstantVelocity\nfrom smartfusion.types.groundtruth import GroundTruthPath, GroundTruthState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate ground truths\n\nGenerate transition model and ground truths as in Tutorial 1.\n\nThe number of targets in this simulation is defined by `ntruths` - here there are 3 targets travelling\nin different directions. The time the\nsimulation is observed for is defined by `time_max`.\n\nWe can fix our random number generator in order to probe a particular example repeatedly. This can be undone by\ncommenting out the first two lines in the next cell.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1990)\nrandom.seed(1990)\n\n# Generate transition model\ntransition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(0.005),\n                                                          ConstantVelocity(0.005)])\n\nyps = range(0, 100, 10)  # y value for prior state\ntruths = []\nntruths = 3  # number of ground truths in simulation\ntime_max = 50  # timestamps the simulation is observed over\n\nxdirection = 1\nydirection = 1\n\n# Generate ground truths\nfor j in range(0, ntruths):\n    truth = GroundTruthPath([GroundTruthState([0, xdirection, yps[j], ydirection], timestamp=start_time)],\n                            id=f\"id{j}\")\n\n    for k in range(1, time_max):\n        truth.append(\n            GroundTruthState(transition_model.function(truth[k - 1], noise=True, time_interval=timedelta(seconds=1)),\n                             timestamp=start_time + timedelta(seconds=k)))\n    truths.append(truth)\n\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the ground truths. This is done using the :class:`~.Plotterly` class from SMART FUSION.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.plotter import Plotterly\n\n# Smartfusion plotter requires sets not lists\ntruths_set = set(truths)\n\nplotter = Plotterly()\nplotter.plot_ground_truths(truths_set, [0, 2])\nplotter.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create sensors\nCreate a set of sensors for each sensor management algorithm. As in Tutorial 1 this tutorial uses the\n:class:`~.RadarRotatingBearingRange` sensor with the\nnumber of sensors initially set as 2. Each sensor is positioned along the line $x=10$, at distance\nintervals of 50.\n\nIncreasing the number of sensors above 2 significantly increases the run time of the sensor manager due to the\nincrease in combinations to be considered by the :class:`~.BruteForceSensorManager`.\nNote that in Tutorial 1 we did not set the resolution for the dwell centre whereas here we are setting it to 30\ndegrees. This is because for the brute force algorithm with multiple sensors, using the default resolution of 1\ndegree is not practical. These limitations due to combinatorics are discussed further later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_no_sensors = 2\n\nfrom smartfusion.types.state import StateVector\nfrom smartfusion.sensor.radar.radar import RadarRotatingBearingRange\nfrom smartfusion.types.angle import Angle\n\nsensor_setA = set()\nfor n in range(0, total_no_sensors):\n    sensor = RadarRotatingBearingRange(\n        position_mapping=(0, 2),\n        noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                              [0, 1 ** 2]]),\n        ndim_state=4,\n        position=np.array([[10], [n * 50]]),\n        rpm=60,\n        fov_angle=np.radians(30),\n        dwell_centre=StateVector([0.0]),\n        max_range=np.inf,\n        resolutions={'dwell_centre': Angle(np.radians(30))}\n    )\n    sensor_setA.add(sensor)\nfor sensor in sensor_setA:\n    sensor.timestamp = start_time\n\nsensor_setB = set()\nfor n in range(0, total_no_sensors):\n    sensor = RadarRotatingBearingRange(\n        position_mapping=(0, 2),\n        noise_covar=np.array([[np.radians(0.5) ** 2, 0],\n                              [0, 1 ** 2]]),\n        ndim_state=4,\n        position=np.array([[10], [n * 50]]),\n        rpm=60,\n        fov_angle=np.radians(30),\n        dwell_centre=StateVector([0.0]),\n        max_range=np.inf,\n        resolutions={'dwell_centre': Angle(np.radians(30))}\n    )\n    sensor_setB.add(sensor)\n\nfor sensor in sensor_setB:\n    sensor.timestamp = start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Kalman predictor and updater\n\nConstruct a predictor and updater using the :class:`~.KalmanPredictor` and :class:`~.ExtendedKalmanUpdater`\ncomponents from SMART FUSION. The measurement model for the updater is `None` as it is an attribute of the sensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.predictor.kalman import KalmanPredictor\npredictor = KalmanPredictor(transition_model)\n\nfrom smartfusion.updater.kalman import ExtendedKalmanUpdater\nupdater = ExtendedKalmanUpdater(measurement_model=None)\n# measurement model is added to detections by the sensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Kalman filters\n\nCreate priors which estimate the targets' initial states - these are the same as in the first\nsensor management tutorial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.types.state import GaussianState\n\npriors = []\nxdirection = 1.2\nydirection = 1.2\nfor j in range(0, ntruths):\n    priors.append(GaussianState([[0], [xdirection], [yps[j]+0.1], [ydirection]],\n                                np.diag([0.5, 0.5, 0.5, 0.5]+np.random.normal(0,5e-4,4)),\n                                timestamp=start_time))\n    xdirection *= -1\n    if j % 2 == 0:\n        ydirection *= -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise the tracks by creating an empty list and appending the priors generated. This needs to be done\nseparately for both sensor manager methods as they will generate different sets of tracks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.types.track import Track\n\n# Initialise tracks from the RandomSensorManager\ntracksA = []\nfor j, prior in enumerate(priors):\n    tracksA.append(Track([prior]))\n\n# Initialise tracks from the BruteForceSensorManager\ntracksB = []\nfor j, prior in enumerate(priors):\n    tracksB.append(Track([prior]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create sensor managers\n\nNext we create our sensor management classes. As in Tutorial 1, two sensor manager classes are used -\n:class:`~.RandomSensorManager` and :class:`~.BruteForceSensorManager`.\n\n#### Random sensor manager\n\nThe first method :class:`~.RandomSensorManager`, randomly chooses the action(s) for the sensors to\ntake to make an observation. To do this the\n:meth:`choose_actions` function uses :meth:`random.choice()` to choose a direction for each\nsensor to observe from the possible actions it can take. It returns the chosen configuration of sensors and\nactions to be taken as a mapping.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.sensormanager import RandomSensorManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Brute force sensor manager\n\nThe second method :class:`~.BruteForceSensorManager` chooses the configuration of sensors and actions which results\nin the greatest reward as calculated by the reward function.\n\nIn this example this is the largest difference between the uncertainty covariances of the target\npredictions and posteriors\nassuming a predicted measurement corresponding to that prediction. This means the sensor manager chooses\nto point the sensors in directions such that the uncertainty will be reduced the most by\nmaking observations in those directions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.sensormanager import BruteForceSensorManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reward function\nThe :class:`UncertaintyRewardFunction` calculates the uncertainty reduction by computing the difference between the\ncovariance matrix norms of the\nprediction and the posterior assuming a predicted measurement corresponding to that prediction. The sum\nof these differences is returned as a metric for that configuration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.sensormanager.reward import UncertaintyRewardFunction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initiate sensor managers\n\nCreate an instance of each sensor manager class. Both sensor managers take in a `sensor_set`.\nThe :class:`~.BruteForceSensorManager` also requires a callable reward function which is initiated here\nfrom the :class:`UncertaintyRewardFunction`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "randomsensormanager = RandomSensorManager(sensor_setA)\n\n# initiate reward function\nreward_function = UncertaintyRewardFunction(predictor, updater)\n\nbruteforcesensormanager = BruteForceSensorManager(sensor_setB,\n                                                  reward_function=reward_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the sensor managers\n\nBoth sensor management methods require a timestamp and a list of tracks at each time step when calling\nthe function :meth:`choose_actions`. This returns a mapping of sensors and actions to be taken by each\nsensor, decided by the sensor managers.\n\nFor both sensor management methods, at each time step the chosen action is given to the sensors and then\nmeasurements taken. The tracks are updated based on these measurements with predictions made for tracks\nwhich have not been observed.\n\nFirst a hypothesiser and data associator are required for use in both trackers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.hypothesiser.distance import DistanceHypothesiser\nfrom smartfusion.measures import Mahalanobis\nhypothesiser = DistanceHypothesiser(predictor, updater, measure=Mahalanobis(), missed_distance=5)\n\nfrom smartfusion.dataassociator.neighbour import GNNWith2DAssignment\ndata_associator = GNNWith2DAssignment(hypothesiser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run random sensor manager\n\nHere the chosen target for observation is selected randomly using the method :meth:`choose_actions()` from the class\n:class:`~.RandomSensorManager`. This returns a mapping of sensors to actions where actions are a\n:class:`~.ChangeDwellAction`, selected at random.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ordered_set import OrderedSet\n\n# Generate list of timesteps from ground truth timestamps\ntimesteps = []\nfor state in truths[0]:\n    timesteps.append(state.timestamp)\n\nfor timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    chosen_actions = randomsensormanager.choose_actions(tracksA, timestep)\n\n    # Create empty dictionary for measurements\n    measurementsA = []\n\n    for chosen_action in chosen_actions:\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(actions)\n\n    for sensor in sensor_setA:\n        sensor.act(timestep)\n\n        # Observe this ground truth\n        measurements = sensor.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n        measurementsA.extend(measurements)\n\n    hypotheses = data_associator.associate(tracksA,\n                                           measurementsA,\n                                           timestep)\n    for track in tracksA:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target. The positions of the sensors are indicated\nby black x markers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotterA = Plotterly()\nplotterA.plot_sensors(sensor_setA)\nplotterA.plot_ground_truths(truths_set, [0, 2])\nplotterA.plot_tracks(set(tracksA), [0, 2], uncertainty=True)\nplotterA.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In comparison to Tutorial 1 the performance of the :class:`~.RandomSensorManager` has improved. This is\nbecause a greater number of sensors means each target is more likely to be observed. This means the uncertainty\nof the track does not increase as much because the targets are observed more often.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run brute force sensor manager\n\nHere the direction for observation is selected based on the difference between the covariance matrices of the\nprediction and posterior, for targets which could be observed by the sensor\npointing in the given direction.\n\nWithin the sensor manager a dictionary is created of sensors and all the possible actions they can take.\nWhen the :meth:`choose_actions` function is called (at each time step), for each track in the tracks list:\n\n * A prediction is made for each track and the covariance matrix norms stored\n * For each possible action a sensor could take, a synthetic detection is made using this sensor configuration\n * A hypothesis is generated based on the stored prediction and synthetic detection\n * This hypothesis is used to do an update and the covariance matrix norms of the update are stored\n * The difference between the covariance matrix norms of the update and the prediction is calculated\n\nThe overall reward is calculated as the sum of the differences between these covariance matrix norms\nfor the tracks observed by the possible action configuration. The sensor manager identifies the\nconfiguration which results in the largest value of this reward and therefore\nlargest reduction in uncertainty. It returns the optimum sensors/actions configuration as a dictionary.\n\nThe actions are given to the sensors, measurements made and\nthe tracks updated based on these measurements. Predictions are made for tracks\nwhich have not been observed by the sensors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for timestep in timesteps[1:]:\n\n    # Generate chosen configuration\n    chosen_actions = bruteforcesensormanager.choose_actions(tracksB, timestep)\n\n    # Create empty dictionary for measurements\n    measurementsB = []\n\n    for chosen_action in chosen_actions:\n        for sensor, actions in chosen_action.items():\n            sensor.add_actions(actions)\n\n    for sensor in sensor_setB:\n        sensor.act(timestep)\n\n        # Observe this ground truth\n        measurements = sensor.measure(OrderedSet(truth[timestep] for truth in truths), noise=True)\n        measurementsB.extend(measurements)\n\n    hypotheses = data_associator.associate(tracksB,\n                                           measurementsB,\n                                           timestep)\n    for track in tracksB:\n        hypothesis = hypotheses[track]\n        if hypothesis.measurement:\n            post = updater.update(hypothesis)\n            track.append(post)\n        else:  # When data associator says no detections are good enough, we'll keep the prediction\n            track.append(hypothesis.prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot ground truths, tracks and uncertainty ellipses for each target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotterB = Plotterly()\nplotterB.plot_sensors(sensor_setB)\nplotterB.plot_ground_truths(truths_set, [0, 2])\nplotterB.plot_tracks(set(tracksB), [0, 2], uncertainty=True)\nplotterB.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The smaller uncertainty ellipses in this plot suggest that the :class:`~.BruteForceSensorManager` provides a much\nbetter track than the :class:`~.RandomSensorManager`.\nThe tracking is also improved from Tutorial 1 due to the increased number of sensors.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combinatorics\n\nThe following graph demonstrates how the number of possible sensor-action configurations increases with the number\nof sensors and number of actions. The number of configurations which are considered by the sensor manager\nfor $M$ actions and $N$ sensors is $M^N$.\n\nWith a resolution of 1 degree there are 360 possible dwell centres for a sensor so the number of\npossible configurations should be $360^N$\nwhere $N$ is the number of sensors. This exponential increase means that as the number of\nsensors increase, the run time of the sensor manager slows down significantly because there are so many more\niterations to consider.\n\nIn this scenario $N=2$ so with a resolution of 1 degree that would 129,600\nactions to consider.\nEven with a resolution of 30 degrees, changing the number of sensors to $N\\geq 2$ leads to a\nmuch longer run time.\nThis highlights a practical limitation of using this brute force optimisation method for multiple\nsensors. In this example we have dealt with this by introducing a run time limit to the brute force sensor manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nnsensors = np.arange(1, 10)\nnactions = np.arange(1, 360.0)\nnsensors, nactions = np.meshgrid(nsensors, nactions)\nncombinations = nactions**nsensors\n\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\nax.plot_surface(nsensors, nactions, np.log10(ncombinations), cmap='coolwarm')\nax.set_xlabel(\"No. sensors\")\nax.set_ylabel(\"No. actions\")\nax.set_zlabel(\"log of no. combinations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics\n\nMetrics can be used to compare how well different sensor management techniques are working.\nAs in Tutorial 1 the metrics used here are the OSPA, SIAP and uncertainty metrics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.metricgenerator.ospametric import OSPAMetric\nospa_generator = OSPAMetric(c=40, p=1)\n\nfrom smartfusion.metricgenerator.tracktotruthmetrics import SIAPMetrics\nfrom smartfusion.measures import Euclidean\nsiap_generator = SIAPMetrics(position_measure=Euclidean((0, 2)),\n                             velocity_measure=Euclidean((1, 3)))\n\nfrom smartfusion.dataassociator.tracktotrack import TrackToTruth\nassociator = TrackToTruth(association_threshold=30)\n\nfrom smartfusion.metricgenerator.uncertaintymetric import SumofCovarianceNormsMetric\nuncertainty_generator = SumofCovarianceNormsMetric()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a metrics manager for each sensor management method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.metricgenerator.manager import SimpleManager\n\nmetric_managerA = SimpleManager([ospa_generator, siap_generator, uncertainty_generator],\n                                associator=associator)\n\nmetric_managerB = SimpleManager([ospa_generator, siap_generator, uncertainty_generator],\n                                associator=associator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each time step, data is added to the metric manager on truths and tracks.\nThe metrics themselves can then be generated from the metric manager.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metric_managerA.add_data(truths, tracksA)\nmetric_managerB.add_data(truths, tracksB)\n\nmetricsA = metric_managerA.generate_metrics()\nmetricsB = metric_managerB.generate_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OSPA metric\n\nFirst we look at the OSPA metric. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ospa_metricA = metricsA['OSPA distances']\nospa_metricB = metricsB['OSPA distances']\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.plot([i.timestamp for i in ospa_metricA.value],\n        [i.value for i in ospa_metricA.value],\n        label='RandomSensorManager')\nax.plot([i.timestamp for i in ospa_metricB.value],\n        [i.value for i in ospa_metricB.value],\n        label='BruteForceSensorManager')\nax.set_ylabel(\"OSPA distance\")\nax.set_xlabel(\"Time\")\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The OSPA distance for the :class:`~.BruteForceSensorManager` is generally smaller than for the random\nobservations of the :class:`~.RandomSensorManager`.\n\n### SIAP metrics\n\nNext we look at SIAP metrics. We are only interested in the positional accuracy (PA) and\nvelocity accuracy (VA). These metrics can be plotted to show how they change over time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2)\n\ntimes = metric_managerA.list_timestamps()\n\npa_metricA = metricsA['SIAP Position Accuracy at times']\nva_metricA = metricsA['SIAP Velocity Accuracy at times']\n\npa_metricB = metricsB['SIAP Position Accuracy at times']\nva_metricB = metricsB['SIAP Velocity Accuracy at times']\n\naxes[0].set(title='Positional Accuracy', xlabel='Time', ylabel='PA')\naxes[0].plot(times, [metric.value for metric in pa_metricA.value],\n             label='RandomSensorManager')\naxes[0].plot(times, [metric.value for metric in pa_metricB.value],\n             label='BruteForceSensorManager')\naxes[0].legend()\n\naxes[1].set(title='Velocity Accuracy', xlabel='Time', ylabel='VA')\naxes[1].plot(times, [metric.value for metric in va_metricA.value],\n             label='RandomSensorManager')\naxes[1].plot(times, [metric.value for metric in va_metricB.value],\n             label='BruteForceSensorManager')\naxes[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the OSPA distance the :class:`~.BruteForceSensorManager` generally results in both a better\npositional accuracy and velocity accuracy than the random observations of the :class:`~.RandomSensorManager`.\n\n### Uncertainty metric\n\nFinally we look at the uncertainty metric which computes the sum of covariance matrix norms of each state at each\ntime step. This is plotted over time for each sensor manager method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "uncertainty_metricA = metricsA['Sum of Covariance Norms Metric']\nuncertainty_metricB = metricsB['Sum of Covariance Norms Metric']\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.plot([i.timestamp for i in uncertainty_metricA.value],\n        [i.value for i in uncertainty_metricA.value],\n        label='RandomSensorManager')\nax.plot([i.timestamp for i in uncertainty_metricB.value],\n        [i.value for i in uncertainty_metricB.value],\n        label='BruteForceSensorManager')\nax.set_ylabel(\"Sum of covariance matrix norms\")\nax.set_xlabel(\"Time\")\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This metric shows that the uncertainty in the tracks generated by the :class:`~.RandomSensorManager` is\ngenerally greater than for those generated by the :class:`~.BruteForceSensorManager`.\nThis is also reflected by the uncertainty ellipses\nin the initial plots of tracks and truths.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [#] *D. Schuhmacher, B. Vo and B. Vo*, **A Consistent Metric for Performance Evaluation of\n   Multi-Object Filters**, IEEE Trans. Signal Processing 2008\n.. [#] *Votruba, Paul & Nisley, Rich & Rothrock, Ron and Zombro, Brett.*, **Single Integrated Air\n   Picture (SIAP) Metrics Implementation**, 2001\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}