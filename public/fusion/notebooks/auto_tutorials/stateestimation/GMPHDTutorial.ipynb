{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gaussian mixture PHD tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background\n\nPrevious tutorials have described the difficulties of state estimation when there are\nmultiple targets under consideration. The probability hypothesis density (PHD) filter has been proposed as a solution\nto this problem that is analogous to the Kalman Filter's solution in single-object\ntracking. Where the Kalman filter propagates the first order movement of the posterior\ndistribution of the target, the PHD filter creates a multiple target posterior\ndistribution and propagates its first-order statistical moment, or PHD. At each\ntime instance, the collections of targets and detections (including both measurements\nand false detections) are modelled as random finite sets. This means that the number\nof elements in each set is a random variable, and the elements themselves follow a\nprobability distribution. Note that this is different from previously discussed filters\nwhich have a constant or known number of objects.\n\nIn a GM-PHD filter, each object is assumed to follow a linear Gaussian model, just like\nwe saw in previous tutorials. However, the multiple objects need not have the same\ncovariance matrices, meaning that the multiple target posterior distribution will be\na **Gaussian mixture (GM)**.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we will recall some of the formulas that will be used in this filter.\n\nTransition Density: Given a state $p(\\mathbf{x}_{k-1})$ at time $k-1$, the probability density of\na transition to the state $p(\\mathbf{x}_k)$ at time $k$ is given by\n$f_{k\\vert k-1}(\\mathbf{x}_{k}\\vert \\mathbf{x}_{k-1})$\n\nLikelihood Function: Given a state $\\mathbf{x}_{k}$ at time $k$, the probability density of\nreceiving the detection $\\mathbf{z}_{k}$ is given by\n$g_{k}(\\mathbf{z}_{k}\\vert \\mathbf{x}_{k})$\n\nThe Posterior Density: The probability density of state $\\mathbf{x}_{k}$ given all the previous\nobservations is denoted by $p_{k}(\\mathbf{x}_{k}\\vert \\mathbf{z}_{1:k})$. Using an initial density\n$p_{0}(\\cdot)$, we can apply Bayes' recursion to show that the posterior density is actually\n\n\\begin{align}p_{k}(\\mathbf{x}_{k}\\vert \\mathbf{z}_{1:k}) = {{g_{k}(\\mathbf{z}_{k}\\vert \\mathbf{x}_{k})p_{k\\vert k-1}(\\mathbf{x}_{k}\\vert \\mathbf{z}_{1:k-1})} \\over {\\int g_{k}(\\mathbf{z}_{k}\\vert \\mathbf{x})p_{k\\vert k-1}(\\mathbf{x}\\vert \\mathbf{z}_{1:k-1})d\\mathbf{x}}}\\end{align}\n\n\nIt is important to notice here that the state at time $k$ can be derived wholly by\nthe state at time $k-1$.\n\nHere we also introduce the following notation:\n$p_{S,k}(\\zeta)$ is the probability that a target $S$ will exist at time $k$ given that\nits previous state was $\\zeta$\n\nSuppose we have the random finite set $\\mathbf{X}_{k} \\in \\chi$ corresponding to the set of\ntarget states at time $k$ and $\\mathbf{X}_k$ has probability distribution $P$. Integrating over\nevery region $S \\in \\chi$, we get a formula for the first order moment (also called the\nintensity) at time $k$, $v_{k}$\n\n\\begin{align}\\int \\left \\vert \\mathbf{X}_{k}\\cap S\\right \\vert P(d\\mathbf{X}_k)=\\int _{S}v_{k}(x)dx.\\end{align}\n\nThe set of targets spawned at time $k$ by a target whose previous state was $\\zeta$ is the\nrandom finite set $\\mathbf{B}_{k|k-1}$. This new set of targets has intensity denoted $\\beta_{k|k-1}$.\n\nThe intensity of the random finite set of births at time $k$ is given by $\\gamma_{k}$.\n\nThe intensity of the random finite set of clutter at time $k$ is given by $\\kappa_{k}$.\n\nThe probability that a state $x$ will be detected at time $k$  is given by $p_{D, k}(x)$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assumptions\nThe GM-PHD filter assumes that each target is independent of one another in both generated\nobservations and in evolution. Clutter is also assumed to be independent of the target\nmeasurements. Finally, we assume that the target locations at a given time step are\ndependent on the multi-target prior density, and their distributions are Poisson. Typically,\nthe target locations are also dependent on previous measurements, but that has been omitted\nin current GM-PHD algorithms.\n\n### Posterior Propagation Formula\nUnder the above assumptions, Vo and Ma [#]_ proved that the posterior intensity can be\npropagated in time using the PHD recursion as follows:\n$\\eqalignno{v _{ k\\vert k-1} (x) =&\\, \\int p_{S,k}(\\zeta)f_{k\\vert k-1} (x\\vert \\zeta)v_{k-1}(\\zeta)d\\zeta\\cr & +\\int \\beta_{k\\vert k-1} (x\\vert \\zeta)v_{k-1}(\\zeta)d\\zeta+\\gamma _{k}(x) & \\cr v_{k} (x) =&\\, \\left[ 1-p_{D,k}(x)\\right]v_{k\\vert k-1}(x)\\cr & +\\!\\!\\sum\\limits _{z\\in Z_{k}} \\!{{ p_{D,k}(x)g_{k}(z\\vert x)}v_{k\\vert k-1}(x) \\over { \\kappa _{k}(z)\\!+\\!\\int p_{D,k}(\\xi)g_{k}(z\\vert \\xi)v_{k\\vert k-1}(\\xi)}} . \\cr &&}$\n\nFor more information about the specific formulas for linear and non-linear Gaussian models,\nplease see Vo and Ma's full paper.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Ground-Based Multi-Target Simulation\nThis simulation will include several targets moving in different directions across the 2D\nCartesian plane. The start locations of each object are random. These start locations are\ncalled priors and are known to the filter, via the density $p_{0}(\\cdot)$ discussed above.\n\nAt each time step, new targets are created and some targets die according to defined\nprobabilities.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start with some imports as usual.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Imports for plotting\nfrom matplotlib import pyplot as plt\nplt.rcParams['figure.figsize'] = (14, 12)\nplt.style.use('seaborn-colorblind')\n# Other general imports\nimport numpy as np\nfrom datetime import datetime, timedelta\nstart_time = datetime.now()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate ground truth\n\nAt the end of the tutorial we will plot the Gaussian mixtures. The ground truth Gaussian\nmixtures are stored in this list where each index refers to an instance in time and holds\nall ground truths at that time step.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "truths_by_time = []\n\n# Create transition model\nfrom smartfusion.models.transition.linear import CombinedLinearGaussianTransitionModel, ConstantVelocity\ntransition_model = CombinedLinearGaussianTransitionModel(\n    (ConstantVelocity(0.3), ConstantVelocity(0.3)))\n\nfrom smartfusion.types.groundtruth import GroundTruthPath, GroundTruthState\nstart_time = datetime.now()\ntruths = set()  # Truths across all time\ncurrent_truths = set()  # Truths alive at current time\nstart_truths = set()\nnumber_steps = 20\ndeath_probability = 0.005\nbirth_probability = 0.2\n\n# Initialize 3 truths. This can be changed to any number of truths you wish.\ntruths_by_time.append([])\nfor i in range(3):\n    x, y = initial_position = np.random.uniform(-30, 30, 2)  # Range [-30, 30] for x and y\n    x_vel, y_vel = (np.random.rand(2))*2 - 1  # Range [-1, 1] for x and y velocity\n    state = GroundTruthState([x, x_vel, y, y_vel], timestamp=start_time)\n\n    truth = GroundTruthPath([state])\n    current_truths.add(truth)\n    truths.add(truth)\n    start_truths.add(truth)\n    truths_by_time[0].append(state)\n\n# Simulate the ground truth over time\nfor k in range(number_steps):\n    truths_by_time.append([])\n    # Death\n    for truth in current_truths.copy():\n        if np.random.rand() <= death_probability:\n            current_truths.remove(truth)\n    # Update truths\n    for truth in current_truths:\n        updated_state = GroundTruthState(\n            transition_model.function(truth[-1], noise=True, time_interval=timedelta(seconds=1)),\n            timestamp=start_time + timedelta(seconds=k))\n        truth.append(updated_state)\n        truths_by_time[k].append(updated_state)\n    # Birth\n    for _ in range(np.random.poisson(birth_probability)):\n        x, y = initial_position = np.random.rand(2) * [120, 120]  # Range [0, 20] for x and y\n        x_vel, y_vel = (np.random.rand(2))*2 - 1  # Range [-1, 1] for x and y velocity\n        state = GroundTruthState([x, x_vel, y, y_vel], timestamp=start_time + timedelta(seconds=k))\n\n        # Add to truth set for current and for all timestamps\n        truth = GroundTruthPath([state])\n        current_truths.add(truth)\n        truths.add(truth)\n        truths_by_time[k].append(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the ground truth\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.plotter import Plotterly\nplotter = Plotterly()\nplotter.plot_ground_truths(truths, [0, 2])\nplotter.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate detections with clutter\nNext, generate detections with clutter just as in the previous tutorial. The clutter is\nassumed to be uniformly distributed across the entire field of view, here assumed to\nbe the space where $x \\in [-200, 200]$ and $y \\in [-200, 200]$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Make the measurement model\nfrom smartfusion.models.measurement.linear import LinearGaussian\nmeasurement_model = LinearGaussian(\n    ndim_state=4,\n    mapping=(0, 2),\n    noise_covar=np.array([[0.75, 0],\n                          [0, 0.75]])\n    )\n\n\n# Generate detections and clutter\n\nfrom scipy.stats import uniform\n\nfrom smartfusion.types.detection import TrueDetection\nfrom smartfusion.types.detection import Clutter\n\nall_measurements = []\n# The probability detection and clutter rate play key roles in the posterior intensity.\n# They can be changed to see their effect.\nprobability_detection = 0.9\nclutter_rate = 3.0\n\nfor k in range(number_steps):\n    measurement_set = set()\n    timestamp = start_time + timedelta(seconds=k)\n\n    for truth in truths:\n        try:\n            truth_state = truth[timestamp]\n        except IndexError:\n            # This truth not alive at this time. Skip this iteration of the for loop.\n            continue\n\n        # Generate actual detection from the state with a 10% chance that no detection is received.\n        if np.random.rand() <= probability_detection:\n            # Generate actual detection from the state\n            measurement = measurement_model.function(truth_state, noise=True)\n            measurement_set.add(TrueDetection(state_vector=measurement,\n                                              groundtruth_path=truth,\n                                              timestamp=truth_state.timestamp,\n                                              measurement_model=measurement_model))\n\n    # Generate clutter at this time-step\n    for _ in range(np.random.poisson(clutter_rate)):\n        x = uniform.rvs(-200, 400)\n        y = uniform.rvs(-200, 400)\n        measurement_set.add(Clutter(np.array([[x], [y]]), timestamp=timestamp,\n                                    measurement_model=measurement_model))\n\n    all_measurements.append(measurement_set)\n\n\n\n# Plot true detections and clutter.\nplotter.plot_measurements(all_measurements, [0, 2])\nplotter.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Predictor and Updater\n\nThe updater is a :class:`~.PHDUpdater`, and since it uses the mixed Gaussian paths, it is a\nGM-PHD updater. For each individual track we use a :class:`~.KalmanUpdater`. Here we assume\nthat the measurement range and clutter spatial density are known to the filter. We\ninvite you to change these variables to mimic removing this assumption.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.updater.kalman import KalmanUpdater\nkalman_updater = KalmanUpdater(measurement_model)\n\n# Area in which we look for target. Note that if a target appears outside of this area the \n# filter will not pick up on it.\nmeas_range = np.array([[-1, 1], [-1, 1]])*200   \nclutter_spatial_density = clutter_rate/np.prod(np.diff(meas_range))\n\nfrom smartfusion.updater.pointprocess import PHDUpdater\nupdater = PHDUpdater(\n    kalman_updater,\n    clutter_spatial_density=clutter_spatial_density,\n    prob_detection=probability_detection,\n    prob_survival=1-death_probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The GM-PHD filter quantifies the predicted-measurement distance, just as in previous\ntutorials. The metric for this is the Mahalanobis distance. We also require two\nobjects which together form the predictor and to generate hypotheses (or predictions)\nbased on the previous state. Recall that the GM-PHD propagates the first-order\nstatistical moment which is a Gaussian mixture. The predicted state at the next time\nstep is also a Gaussian mixture and can be determined solely by the propagated prior.\nDetermining this predicted Gaussian mixture is the job for the\n:class:`~.GaussianMixtureHypothesiser` class. We must also generate a prediction for each track\nin the simulation, and so use the :class:`~.DistanceHypothesiser` object as before.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.predictor.kalman import KalmanPredictor\nkalman_predictor = KalmanPredictor(transition_model)\n\nfrom smartfusion.hypothesiser.distance import DistanceHypothesiser\nfrom smartfusion.measures import Mahalanobis\nbase_hypothesiser = DistanceHypothesiser(kalman_predictor, kalman_updater, Mahalanobis(), missed_distance=3)\n\nfrom smartfusion.hypothesiser.gaussianmixture import GaussianMixtureHypothesiser\nhypothesiser = GaussianMixtureHypothesiser(base_hypothesiser, order_by_detection=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The updater takes a list of hypotheses from the hypothesiser and transforms them into\npotential new states for our tracks. Each state is a :class:`~.TaggedWeightedGaussianState`\nobject and has a state vector, covariance, weight, tag, and timestamp. Some of the\nupdated states have a very low weight, indicating that they do not contribute much to\nthe Gaussian mixture. To ease the computational complexity, a :class:`~.GaussianMixtureReducer`\nis used to merge and prune many of the states based on provided thresholds. States whose\ndistance is less than the merging threshold will be combined, and states whose weight\nis less than the pruning threshold will be removed. Additionally, the\n:class:`~.GaussianMixtureReducer` has an optional parameter for the maximum number of \ncomponents that will be kept in the mixture, `max_number_components`. The reducer will keep\nonly the `max_number_components` components with the highest weights. This threshold can be\nused when the approximate number of targets is known, or when there is high uncertainty and it\nis hard to decide on a pruning threshold. It will not be used in this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.mixturereducer.gaussianmixture import GaussianMixtureReducer\n# Initialise a Gaussian Mixture reducer\nmerge_threshold = 5\nprune_threshold = 1E-8\n\nreducer = GaussianMixtureReducer(\n    prune_threshold=prune_threshold,\n    pruning=True,\n    merge_threshold=merge_threshold,\n    merging=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we initialize the Gaussian mixture at time k=0. In this implementation, the GM-PHD\ntracker knows the start state of the first 3 tracks that were created. After that it\nmust pick up on new tracks and discard old ones. It is not necessary to provide the \ntracker with these start states, you can simply define the `tracks` as an empty set.\n\nFeel free to change the `state_vector` from the actual truth state vector to something\nelse. This would mimic if the tracker was unsure about where the objects were originating.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from smartfusion.types.state import TaggedWeightedGaussianState\nfrom smartfusion.types.track import Track\nfrom smartfusion.types.array import CovarianceMatrix\ncovar = CovarianceMatrix(np.diag([10, 5, 10, 5]))\n\ntracks = set()\nfor truth in start_truths:\n    new_track = TaggedWeightedGaussianState(\n            state_vector=truth.state_vector,\n            covar=covar**2,\n            weight=0.25,\n            tag=TaggedWeightedGaussianState.BIRTH,\n            timestamp=start_time)\n    tracks.add(Track(new_track))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The hypothesiser takes the current Gaussian mixture as a parameter. Here we will\ninitialize it to use later. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reduced_states = set([track[-1] for track in tracks])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To ensure that new targets get represented in the filter, we must add a birth \ncomponent to the Gaussian mixture at every time step. The birth component's mean and \ncovariance must create a distribution that covers the entire state space, and its weight \nmust be equal to the expected number of births per timestep. For more information about \nthe birth component, see the algorithm provided in [#]_. If the state space is very \nlarge, it becomes inefficient to hold a component that covers it. Alternative \nimplementations (as well as more discussion about the birth component) are discussed in\n[#]_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "birth_covar = CovarianceMatrix(np.diag([1000, 2, 1000, 2]))\nbirth_component = TaggedWeightedGaussianState(\n    state_vector=[0, 0, 0, 0],\n    covar=birth_covar**2,\n    weight=0.25,\n    tag='birth',\n    timestamp=start_time\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Tracker\nNow that we have all of our components, we can create a tracker. At each time instance,\nthe tracker will go through four steps: hypothesise, update, reduce, and match. Let us \nbriefly recap these four steps. The 'hypothesise' step is similar to the 'prediction' \nstep in other filters. It uses the existing state space and the measurements to generate \na list of all hypotheses for this time step (remember, each hypothesis is a Gaussian \ncomponent). In the 'update' step, the filter combines the hypotheses into an updated \nGaussian mixture. The 'reduce' step helps limit the computational complexity by merging \nand pruning the updated Gaussian mixture. The filter returns this final set of states \nand then we perform a 'match' step where we use the states' tags to match them with an \nexisting track (or create a new track).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These lists will be used to plot the Gaussian mixtures later. They are not used in\nthe filter itself.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_gaussians = []\ntracks_by_time = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need a threshold to compare state weights against. If the state has a high enough\nweight in the Gaussian mixture, we will add it to an existing track or make a new\ntrack for it. Lowering this value makes the filter more sensitive but may also\nincrease the number of false estimations. Increasing this value may increase the number\nof missed targets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state_threshold = 0.25\n\nfor n, measurements in enumerate(all_measurements):\n    tracks_by_time.append([])\n    all_gaussians.append([])\n\n    # The hypothesiser takes in the current state of the Gaussian mixture. This is equal to the list of \n    # reduced states from the previous iteration. If this is the first iteration, then we use the priors \n    # defined above. \n    current_state = reduced_states\n    \n    # At every time step we must add the birth component to the current state \n    if measurements: \n        time = list(measurements)[0].timestamp\n    else: \n        time = start_time + timedelta(seconds=n)\n    birth_component.timestamp = time\n    current_state.add(birth_component)\n\n    # Generate the set of hypotheses\n    hypotheses = hypothesiser.hypothesise(current_state,\n                                          measurements,\n                                          timestamp=time,\n                                          # keep our hypotheses ordered by detection, not by track\n                                          order_by_detection=True)\n\n    # Turn the hypotheses into a GaussianMixture object holding a list of states\n    updated_states = updater.update(hypotheses)\n\n    # Prune and merge the updated states into a list of reduced states\n    reduced_states = set(reducer.reduce(updated_states))\n\n    # Add the reduced states to the track list. Each reduced state has a unique tag. If this tag matches the tag of a\n    # state from a live track, we add the state to that track. Otherwise, we generate a new track if the reduced\n    # state's weight is high enough (i.e. we are sufficiently certain that it is a new track).\n    for reduced_state in reduced_states:\n        # Add the reduced state to the list of Gaussians that we will plot later. Have a low threshold to eliminate some\n        # clutter that would make the graph busy and hard to understand\n        if reduced_state.weight > 0.05: all_gaussians[n].append(reduced_state)\n\n        tag = reduced_state.tag\n        # Here we check to see if the state has a sufficiently high weight to consider being added.\n        if reduced_state.weight > state_threshold:\n            # Check if the reduced state belongs to a live track\n            for track in tracks:\n                track_tags = [state.tag for state in track.states]\n\n                if tag in track_tags:\n                    track.append(reduced_state)\n                    tracks_by_time[n].append(reduced_state)\n                    break\n            else:  # Execute if no \"break\" is hit; i.e. no track with matching tag\n                # Make a new track out of the reduced state\n                new_track = Track(reduced_state)\n                tracks.add(new_track)\n                tracks_by_time[n].append(reduced_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the Tracks\nFirst, determine the x and y range for axes. We want to zoom in as much as possible\non the measurements and tracks while not losing any of the information. This section\nis not strictly necessary as we already set the field of view to be the range [-100, 100]\nfor both x and y. However, sometimes an object may leave the field of view. If you want\nto ignore objects that have left the field of view, comment out this section and define\nthe variables\n`x_min` = `y_min` = -100 and `x_max` = `y_max` = 100.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_min, x_max, y_min, y_max = 0, 0, 0, 0\n\n# Get bounds from the tracks\nfor track in tracks:\n    for state in track:\n        x_min = min([state.state_vector[0], x_min])\n        x_max = max([state.state_vector[0], x_max])\n        y_min = min([state.state_vector[2], y_min])\n        y_max = max([state.state_vector[2], y_max])\n\n# Get bounds from measurements\nfor measurement_set in all_measurements:\n    for measurement in measurement_set:\n        if type(measurement) == TrueDetection:\n            x_min = min([measurement.state_vector[0], x_min])\n            x_max = max([measurement.state_vector[0], x_max])\n            y_min = min([measurement.state_vector[1], y_min])\n            y_max = max([measurement.state_vector[1], y_max])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use the :class:`~.Plotter` class to draw the tracks. Note that if the birth\ncomponent it plotted you will see its uncertainty ellipse centred around $(0, 0)$.\nThis ellipse need not cover the entire state space, as long as the distribution does.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot the tracks\nplotter = Plotterly()\nplotter.plot_ground_truths(truths, [0, 2])\nplotter.plot_measurements(all_measurements, [0, 2])\nplotter.plot_tracks(tracks, [0, 2], uncertainty=True)\nplotter.fig.update_xaxes(range=[x_min-5, x_max+5])\nplotter.fig.update_yaxes(range=[y_min-5, y_max+5])\nplotter.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examining the Gaussian Mixtures\nAt every time step, the above GM-PHD algorithm creates a Gaussian mixture, which is\na distribution over our target space. The following sections take a closer look at what\nthis Gaussian really looks like. Note that the figures below only include the reduced\nstates which have weight greater than 0.05. This decreases the overall number of states\nshown in the mixture and makes it easier to examine. But you can change this threshold\nparameter in the Run The Tracker section.\n\nFirst we define a function that will help generate the z values for the Gaussian\nmixture. This lets us plot it later. This function has been updated from the one\nfound [here](https://notebook.community/empet/Plotly-plots/Gaussian-Mixture) from \n[this](https://github.com/empet/Plotly-plotse) GPL-3.0 licensed repository.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\ndef get_mixture_density(x, y, weights, means, sigmas):\n    # We use the quantiles as a parameter in the multivariate_normal function. We don't need to pass in any quantiles,\n    # but the last axis must have the components x and y\n    quantiles = np.empty(x.shape + (2,))  # if  x.shape is (m,n) then quantiles.shape is (m,n,2)\n    quantiles[:, :, 0] = x\n    quantiles[:, :, 1] = y\n\n    # Go through each gaussian in the list and add its PDF to the mixture\n    z = np.zeros(x.shape)\n    for gaussian in range(len(weights)):\n        z += weights[gaussian]*multivariate_normal.pdf(x=quantiles, mean=means[gaussian, :], cov=sigmas[gaussian])\n    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each timestep, create a new figure with 2 subplots. The plot on the left will\nshow the 3D Gaussian Mixture density. The plot on the right will show a 2D\nbirds-eye-view of the space, including the ground truths, detections, clutter, and tracks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import animation\nfrom matplotlib import pyplot as plt\nfrom matplotlib.lines import Line2D  # Will be used when making the legend\n\n# This is the function that updates the figure we will be animating. As parameters we must\n# pass in the elements that will be changed, as well as the index i\ndef animate(i, sf, truths, tracks, measurements, clutter):\n    # Set up the axes\n    axL.clear()\n    axR.set_title('Tracking Space at k='+str(i))\n    axL.set_xlabel(\"x\")\n    axL.set_ylabel(\"y\")\n    axL.set_title('PDF of the Gaussian Mixture')\n    axL.view_init(elev=30, azim=-80)\n    axL.set_zlim(0, 0.3)\n\n    # Initialize the variables\n    weights = []  # weights of each Gaussian. This is analogous to the probability of its existence\n    means = []    # means of each Gaussian. This is equal to the x and y of its state vector\n    sigmas = []   # standard deviation of each Gaussian.\n\n    # Fill the lists of weights, means, and standard deviations\n    for state in all_gaussians[i]:\n        weights.append(state.weight)\n        means.append([state.state_vector[0], state.state_vector[2]])\n        sigmas.append([state.covar[0][0], state.covar[1][1]])\n    means = np.array(means)\n    sigmas = np.array(sigmas)\n\n    # Generate the z values over the space and plot on the left axis\n    zarray[:, :, i] = get_mixture_density(x, y, weights, means, sigmas)\n    sf = axL.plot_surface(x, y, zarray[:, :, i], cmap=cm.RdBu, linewidth=0, antialiased=False)\n\n    # Make lists to hold the new ground truths, tracks, detections, and clutter\n    new_truths, new_tracks, new_measurements, new_clutter = [], [], [], []\n    for truth in truths_by_time[i]:\n        new_truths.append([truth.state_vector[0], truth.state_vector[2]])\n    for state in tracks_by_time[i]:\n        new_tracks.append([state.state_vector[0], state.state_vector[2]])\n    for measurement in all_measurements[i]:\n        if isinstance(measurement, TrueDetection):\n            new_measurements.append([measurement.state_vector[0], measurement.state_vector[1]])\n        elif isinstance(measurement, Clutter):\n            new_clutter.append([measurement.state_vector[0], measurement.state_vector[1]])\n\n    # Plot the contents of these lists on the right axis\n    if new_truths:\n        truths.set_offsets(new_truths)\n    if new_tracks:\n        tracks.set_offsets(new_tracks)\n    if new_measurements:\n        measurements.set_offsets(new_measurements)\n    if new_clutter:\n        clutter.set_offsets(new_clutter)\n\n    # Create a legend. The use of Line2D is purely for the visual in the legend\n    data_types = [Line2D([0], [0], color='white', marker='o', markerfacecolor='blue', markersize=15,\n                         label='Ground Truth'),\n                  Line2D([0], [0], color='white', marker='o', markerfacecolor='orange', markersize=15,\n                         label='Clutter'),\n                  Line2D([0], [0], color='white', marker='o', markerfacecolor='green', markersize=15,\n                         label='Detection'),\n                  Line2D([0], [0], color='white', marker='o', markerfacecolor='red', markersize=15,\n                         label='Track')]\n    axR.legend(handles=data_types, bbox_to_anchor=(1.0, 1), loc='upper left')\n\n    return sf, truths, tracks, measurements, clutter\n\n# Set up the x, y, and z space for the 3D axis\nxx = np.linspace(x_min-5, x_max+5, 100)\nyy = np.linspace(y_min-5, y_max+5, 100)\nx, y = np.meshgrid(xx, yy)\nzarray = np.zeros((100, 100, number_steps))\n\n# Create the matplotlib figure and axes. Here we will have two axes being animated in sync.\n# `axL` will be the a 3D axis showing the Gaussian mixture\n# `axR` will be be a 2D axis showing the ground truth, detections, and updated tracks at \n# each time step. \nfig = plt.figure(figsize=(16, 8))\naxL = fig.add_subplot(121, projection='3d')\naxR = fig.add_subplot(122)\naxR.set_xlim(x_min-5, x_max+5)\naxR.set_ylim(y_min-5, y_max+5)\n\n# Add an initial surface to the left axis and scattered points on the right axis. Doing\n# this now means that in the animate() function we only have to update these variables\nsf = axL.plot_surface(x, y, zarray[:, :, 0], cmap=cm.RdBu, linewidth=0, antialiased=False)\ntruths = axR.scatter(x_min-10, y_min-10, c='blue', linewidth=6, zorder=0.5)\ntracks = axR.scatter(x_min-10, y_min-10, c='red', linewidth=4, zorder=1)\nmeasurements = axR.scatter(x_min-10, y_min-10, c='green', linewidth=4, zorder=0.5)\nclutter = axR.scatter(x_min-10, y_min-10, c='orange', linewidth=4, zorder=0.5)\n\n# Create and display the animation\nfrom matplotlib import rc\nanim = animation.FuncAnimation(fig, animate, frames=number_steps, interval=500,\n                               fargs=(sf, truths, tracks, measurements, clutter), blit=False)\nrc('animation', html='jshtml')\nanim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [#] B. Vo and W. Ma, \"The Gaussian Mixture Probability Hypothesis Density Filter,\" in IEEE \n       Transactions on Signal Processing, vol. 54, no. 11, pp. 4091-4104, Nov. 2006, doi: \n       10.1109/TSP.2006.881190\n\n.. [#] D. E. Clark, K. Panta and B. Vo, \"The GM-PHD Filter Multiple Target Tracker,\" 2006 9th \n       International Conference on Information Fusion, 2006, pp. 1-8, doi: 10.1109/ICIF.2006.301809\n\n.. [#] B. Ristic, D. Clark, B. Vo and B. Vo, \"Adaptive Target Birth Intensity for PHD and CPHD \n       Filters,\" in IEEE Transactions on Aerospace and Electronic Systems, vol. 48, no. 2, pp. \n       1656-1668, Apr 2012, doi: 10.1109/TAES.2012.6178085\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}